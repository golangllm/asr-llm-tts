package ws

import (
	"context"
	"log"
	"strings"
	"time"

	"golang.org/x/sync/errgroup"
)

// text->llm->text，text->llm->(text and tts), asr->llm->text, asr->llm->(text and tts)
type SessionMessageType string

const (
	SessionText  SessionMessageType = "text"
	SessionAudio SessionMessageType = "audio"
)

type SessionMessage struct {
	Type SessionMessageType
	Data []byte
}

type Session struct {
	ctx    context.Context
	cancel context.CancelFunc
	g      *errgroup.Group

	audioIn  chan []byte
	textIn   chan string
	ttsInput chan string
	// textOut  chan string
	// audioOut chan []byte
	output       chan SessionMessage
	FullResponse strings.Builder
	// 新增：用于等待 runPipeline 退出
	// done chan struct{}
}

func NewSession() *Session {
	ctx, cancel := context.WithCancel(context.Background())
	g, ctx := errgroup.WithContext(ctx) //  关键：绑定 context

	s := &Session{
		ctx:    ctx,
		cancel: cancel,
		g:      g,
		// textOut:  make(chan string, 32),
		ttsInput: make(chan string, 32),
		// audioOut: make(chan []byte, 32),
		output:  make(chan SessionMessage, 32),
		textIn:  make(chan string, 32),
		audioIn: make(chan []byte, 32),
		// done:    make(chan struct{}), // 标记 runPipeline 是否退出

	}

	// 启动处理协程
	// go s.runPipeline()
	return s
}

// func (s *Session) RunPipeline() {
// 	for {
// 		select {
// 		// case t := <-s.textIn:
// 		// 	// 处理文本 → LLM → TextOut
// 		// 	// out := processLLM(t) // 假设返回 string
// 		// 	// s.output <- SessionMessage{Type: SessionText, Data: []byte(out)}

// 		// 	// // 可选 TTS
// 		// 	// audio := processTTS(out)
// 		// 	// if audio != nil {
// 		// 	// 	s.output <- SessionMessage{Type: SessionAudio, Data: audio}
// 		// 	// }
// 		// 	// go s.processLLMStream(t, s.ctx)
// 		// 	// go s.processLLMStream(t)
// 		// 	// time.Sleep(3 * time.Second)
// 		// 	// s.output <- SessionMessage{Type: SessionText, Data: []byte(t)}

// 		// 	return
// 		// case a := <-s.audioIn:

// 		// 	s.output <- SessionMessage{Type: SessionAudio, Data: a}

// 		case <-s.ctx.Done():
// 			// close(s.output)
// 			return
// 		}
// 	}
// }

// func (s *Session) processLLMStream(inputText string) {
// 	// 假设使用某种流式调用 LLM（例如 OpenAI stream API）
// 	// 模拟流式生成（可以根据实际情况进行实现）
// 	// LLM API 调用部分

// 	// 假设 `LLMStream` 是一个流式 API 返回的文本片段
// 	LLMStream(inputText, func(chunk string) {
// 		// 当 LLM 返回一个片段时，发送到 output
// 		s.output <- SessionMessage{
// 			Type: SessionText,
// 			Data: []byte(chunk),
// 		}
// 	})

// 	// 在 LLM 流式返回完毕后，还可以执行其他逻辑，比如 TTS 转换等
// }

func (s *Session) RunProcessTTsStream() {
	s.g.Go(func() error {
		s.processTTsStream()
		return nil
	})
}

func (s *Session) processTTsStream() {
	for {
		select {
		case message, ok := <-s.ttsInput:
			if !ok {
				log.Println("TTSStream completed")

				return
			}
			log.Println("Start TTSStream for text:", message)
			TTSStream(message, s.ctx, func(chunk []byte) {
				s.sendSafe(SessionAudio, chunk)
				// select {
				// case <-s.ctx.Done():
				// 	return
				// case s.output <- chunk:
				// }
			})
		case <-s.ctx.Done():
			return
		}
	}
}

func TTSStream(text string, ctx context.Context, onChunkReceived func([]byte)) {
	// 假设这是流式调用 LLM API 并且逐步接收响应的实现
	// 这里可以是调用 OpenAI Stream API 或其他模型的流式响应

	// 模拟 LLM 输出逐步返回数据的过程
	parts := [][]byte{[]byte("PMC1"), []byte("PMC2")} // 假设这是 LLM 返回的逐步文本片段

	for _, part := range parts {
		select {
		case <-ctx.Done():
			// 如果收到取消信号，则停止流式调用
			return
		default:
			// 模拟延迟并返回文本片段
			time.Sleep(1 * time.Second)
			onChunkReceived(part) // 返回每个文本片段
			// log.Printf("LLMStream sent chunk: %s", part)
		}
	}
}

func (s *Session) ProcessLLMStream(inputText string) {
	// 假设使用某种流式调用 LLM（例如 OpenAI stream API）
	// 模拟流式生成（可以根据实际情况进行实现）
	// LLM API 调用部分

	// 假设 `LLMStream` 是一个流式 API 返回的文本片段
	LLMStream(inputText, s.ctx, func(chunk string) {
		s.sendSafe(SessionText, []byte(chunk))
		s.FullResponse.WriteString(chunk)

		select {
		case <-s.ctx.Done():
			return
		case s.ttsInput <- chunk:
		}
		// select {
		// case <-s.ctx.Done(): // Check if the session context is done
		// 	log.Println("Close llm Stream!")
		// 	return
		// case s.output <- SessionMessage{
		// 	Type: SessionText,
		// 	Data: []byte(chunk),
		// }:
		// 	// Successfully sent data
		// }
	})
	close(s.ttsInput)

	// 可选：检查上下文是否被取消
	// if s.ctx.Err() != nil {
	// 	log.Printf("Session %s cancelled, skipping DB save", s.ID)
	// 	return
	// }

	// 获取完整响应（此时才转为 string）
	finalResponse := s.FullResponse.String()
	log.Println("Final LLM Response:", finalResponse)

	// 存入数据库
	// err := s.saveToDatabase(inputText, finalResponse)
	// if err != nil {
	// 	log.Printf("DB save failed for session %s: %v", s.ID, err)
	// }
	// s.cancel()
	// 在 LLM 流式返回完毕后，还可以执行其他逻辑，比如 TTS 转换等
}
func LLMStream(inputText string, ctx context.Context, onChunkReceived func(string)) {
	// 假设这是流式调用 LLM API 并且逐步接收响应的实现
	// 这里可以是调用 OpenAI Stream API 或其他模型的流式响应

	// 模拟 LLM 输出逐步返回数据的过程
	parts := []string{"这是", "一些", "文本", "输出", "的", "示例"} // 假设这是 LLM 返回的逐步文本片段

	for _, part := range parts {
		select {
		case <-ctx.Done():
			// 如果收到取消信号，则停止流式调用
			return
		default:
			// 模拟延迟并返回文本片段
			time.Sleep(1 * time.Second)
			onChunkReceived(part) // 返回每个文本片段
			// log.Printf("LLMStream sent chunk: %s", part)
		}
	}
	log.Println("LLMStream completed")
}

func (s *Session) sendSafe(msgType SessionMessageType, data []byte) {
	select {
	case s.output <- SessionMessage{Type: msgType, Data: data}:
	case <-s.ctx.Done():
		// session 已关闭，丢弃消息
	}
}
func (s *Session) PushText(text string) {
	select {
	case s.textIn <- text:
	case <-s.ctx.Done():
	}
}

func (s *Session) PushAudio(audio []byte) {
	select {
	case s.audioIn <- audio:
	case <-s.ctx.Done():
	}
}
func (s *Session) TryPushAudio(audio []byte) bool {
	select {
	case s.audioIn <- audio:
		return true
	case <-s.ctx.Done():
		return false
	default:
		return false // channel full
	}
}

// func (s *Session) Output() <-chan SessionMessage {
// 	return s.output
// }

func (s *Session) Close() {

	// _ = s.g.Wait() // 等待所有协程退出
	if err := s.g.Wait(); err != nil {
		log.Printf("Session ended with error: %v\n", err)
	} else {
		log.Println("All streams completed successfully")
	}
	// s.cancel()
	close(s.output)
}

// <-s.done
// close(s.textIn)
// close(s.audioIn)
// close(s.output)

// func NewSession(asr asr.ASRProvider, llm llm.LLMProvider, tts tts.TTSProvider) *Session {
// 	ctx, cancel := context.WithCancel(context.Background())

// 	audioIn := make(chan []byte)

// 	asrText, _ := asr.Stream(ctx, audioIn)

// 	llmTextIn := make(chan string)
// 	llmText, _ := llm.Stream(ctx, llmTextIn)

// 	ttsAudio, _ := tts.Stream(ctx, llmText)

// 	// ASR → LLM
// 	go func() {
// 		defer close(llmTextIn)
// 		for t := range asrText {
// 			llmTextIn <- t
// 		}
// 	}()

// 	return &Session{
// 		ctx:      ctx,
// 		cancel:   cancel,
// 		audioIn:  audioIn,
// 		textOut:  llmText,
// 		audioOut: ttsAudio,
// 	}
// }
